{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark \n",
    "## Optimal Partitioning for Spark Parquet Files \n",
    "Parquet is a columnar data format commonly used in Big Data contexts. It's columnn-wise data structure can be partitioned allowing for parallelism and optimized query performance through an SQL like interface through the Spark API. \n",
    "\n",
    "An informal literature search revealed a number of strategies to optimize read/write and query performance. This is a late-night summary of the broad strokes. \n",
    "\n",
    "### Motivation\n",
    "The Alibaba Ad Display Click Dataset expands from 255 MB compressed to 24 Gb across the four files summarized below.\n",
    "\n",
    "{numref}`ali_display_summary`: Alibaba Ad Display / Click Summary\n",
    "\n",
    "```{table} Dataset Summary\n",
    ":name: ali_display_summary\n",
    "\n",
    "|        Users        |    User Profiles    |     Impressions      |            Advertising Campaigns           |       Behaviors       |\n",
    "|:-------------------:|:-------------------:|:--------------------:|:------------------------------------------:|:---------------------:|\n",
    "|          1,140,000  |          1,061,768  |          26,557,961  |                                   846,811  |          723,268,134  |\n",
    "```\n",
    "### Dynamic Repartition\n",
    "This captures one approach for processing and persisting medium to big data using Spark and Parquet. Spark documentation describes the DataFrame API and a series of objects, methods and configuration parameters for creating performant, partitioned and parallelizable file representations. Three parameters are central to task:\n",
    "\n",
    "| Property   Name                   | Default                   | Meaning                                                                                                                                                                                                                                                                                                                                                                                                                        | Since Version |   |\n",
    "|-----------------------------------|---------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|---|\n",
    "| spark.sql.files.maxPartitionBytes | 134217728 (128 MB)        | The maximum number of bytes to pack into a single partition   when reading files. This configuration is effective only when using   file-based sources such as Parquet, JSON and ORC.                                                                                                                                                                                                                                          | 2.0.0         |   |\n",
    "| spark.sql.files.openCostInBytes   | 4194304 (4 MB)            | The estimated cost to open a file, measured by the number of   bytes could be scanned in the same time. This is used when putting multiple   files into a partition. It is better to over-estimated, then the partitions   with small files will be faster than partitions with bigger files (which is   scheduled first). This configuration is effective only when using file-based   sources such as Parquet, JSON and ORC. | 2.0.          |   |\n",
    "| spark.default.parallelism         | Default   number of cores | For distributed shuffle operations   like reduceByKey and join, the largest number of   partitions in a parent RDD. For operations like parallelize with no parent   RDDs, it depends on the cluster manager:                                                                                                                                                                                                                  |               |   |\n",
    "In addition, the maxRecordsPerFile option, added in Spark 2.2, allows one to limit the size of the files produced during the partitioning process. This method dynamically scales the file storage configuration to the size of the data partitions. Let's walk through the numbers.\n",
    "\n",
    "#### Configuration Parameters\n",
    "The Spark defaults are:\n",
    "\n",
    "- default.parallelism: The number of CPU Cores (24)\n",
    "- maxPartitionBytes: 128 MB. \n",
    "- openCostInBytes: 4 MB : An estimated cost to open a file. Used to as sort of a minimum filesize for partitioning calculations\n",
    "\n",
    "The maxRecordsPerFile limits the combinatorial explosion of files that can occur with large datasets and repartitions. It is computed as follows:     \n",
    "\n",
    "```{python}\n",
    "maxRecordsPerFile = fileSizeInBytes / maxFileBytes \n",
    "\n",
    "and\n",
    "\n",
    "maxFileBytes = min(maxPartitionBytes, max(openCostInBytes,  bytesPerCore))\n",
    "\n",
    "where\n",
    "\n",
    "bytesPerCore = fileSizeInBytes / default.parallelism   \n",
    "```\n",
    "\n",
    "#### Partition on Disk with partitionBy\n",
    "Spark allows one to partition data on disk based upon the value of a column in the dataset using the petitionBy method. Ideally, we would choose a column with high cardinality and reasonably balanced distribution so that the partitions are of roughly the same size. \n",
    "\n",
    "Our impressions dataset has a timestamp, from which we can extract the day of the month as a partitioning variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "    def execute(self, data: Any = None, context: dict = None) -> pd.DataFrame:\n",
    "        \"\"\"Extracts temporal data from timestamp and adds as columns to data.\"\"\"\n",
    "        data = data.withColumn(\"day\", dayofmonth(timestamp_seconds(col(\"timestamp\"))))\n",
    "\n",
    "        print(data.show())\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Number of Rows per Partition File\n",
    "Next, we create a function to compute maxRecordsPerFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def records_per_file(filesize: int, num_rows: int, cores: int = 24, open_cost: int = 4194304, max_partition_size: int = 134217728) -> int:\n",
    "    \"\"\"Computes the maximum records per parquet partition file\n",
    "\n",
    "    Args:\n",
    "        filesize (int): The size of the file to be partitioned in bytes\n",
    "        num_rows (int): The number of rows in the file\n",
    "        cores (int): Number of CPU Cores. \n",
    "        open_cost (int): Cost to open a partition\n",
    "        max_partition_size (int): Maximum size of a partition in bytes.\n",
    "    Returns (int): maximum number of records per partition file\n",
    "    \"\"\"\n",
    "    # Compute memory per core\n",
    "    bytes_per_core = filesize / cores\n",
    "    # Maximum number of bytes per file\n",
    "    max_file_bytes = min(max_partition_size, max(open_cost, bytes_per_core))\n",
    "    # Row size in bytes\n",
    "    row_size_in_bytes = num_rows / filesize\n",
    "    \n",
    "    return  int(filesize / row_size_in_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Partitioned File\n",
    "We have the maximum number of records per partition file and the partition column.  Next, we repartition the dataset on the partition column and a repartition seed. The product of the number of partitions and a random float [0,1] to control the size of and the number of records in the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filesize = os.stat(data).st_size\n",
    "num_rows = data.count()\n",
    "rows_per_file = records_per_file(filesize=filesize, num_rows=num_rows)\n",
    "partition_columns = ['day']\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "partition_count = data.groupBy(partition_columns).count()\n",
    "\n",
    "data = (\n",
    "    data\n",
    "    .join(partition_count, on=partition_columns)\n",
    "    .withColumn('repartition_seed',(rand() * partition_count['count'] / rows_per_file).cast('int'))\n",
    "    .repartition(*partition_columns, 'repartition_seed')\n",
    "    .write.mode(\"overwrite\").parquet(filepath)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
